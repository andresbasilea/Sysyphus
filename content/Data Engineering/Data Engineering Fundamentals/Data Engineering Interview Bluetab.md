---
draft: true
---

¿Qué buscamos?

**Conocimientos Técnicos:**
- Dominio de los servicios y soluciones de GCP (Compute Engine, App Engine, Cloud Storage, BigQuery, Looker).
- Experiencia en arquitectura de nube, incluyendo redes, virtualización, identidad, seguridad, business continuity, recuperación de desastres y gobernanza de datos.
- Conocimientos en contenedores y su orquestación.
_-_ Conocimientos en bases de datos cloud y herramientas de replicación de datos
- Comprensión de principios de DevOps y herramientas asociadas (CI/CD, monitoreo, logging).

**Habilidades de Implementación y Desarrollo:**
- Capacidad para diseñar, desarrollar e implementar soluciones escalables, seguras y rentables en GCP.
- Experiencia en la migración de aplicaciones, datos y servicios desde entornos locales o de otras nubes a GCP.
- Conocimiento en el uso de herramientas de automatización e infraestructura como código (Terraform, Cloud Deployment Manager).

**Certificaciones y Educación:**
- Certificaciones relevantes de GCP, como Google Cloud Certified - Professional Cloud Architect o Google Cloud Certified - Professional Data Engineer.
- Formación en informática, ingeniería de sistemas o campos relacionados.

**Seguridad y Conformidad:**
- Experiencia en la configuración y gestión de políticas de seguridad en GCP.

**Optimización y Soporte:**
- Habilidad para optimizar costos y rendimiento de las soluciones en GCP.
- Experiencia en proporcionar soporte técnico y resolver problemas en entornos de GCP.

**Se valorará de forma especial:**

- Apasionado de la tecnología. Completamente al día de las últimas tendencias de la industria.- Habituado a trabajar por objetivos. Acostumbrado a trabajar bajo presión en entornos altamente exigentes.- Buen comunicador, sabrá transmitir ilusión al entorno de trabajo.- Comprometido. Capacidad de análisis y solución de problemas.- Buenas habilidades de negociación.- Capaz de generar empatía tanto con los empleados y clientes.- Capaz de adaptarse a la cultura de trabajo de una pyme. Sin grandes jerarquías. Compatibilizando la capacidad de trabajar con autonomía con un fuerte espíritu de colaboración y, sobre todo, de trabajo en equipo.- Interesado en desarrollar su carrera profesional en entornos altamente competitivos y con alto potencial de crecimiento.

**Perfil del candidato/a ideal:**

- Eres un apasionado/a de la tecnología y estás siempre al día con las últimas tendencias de la industria.- Estás acostumbrado/a a trabajar por objetivos y en entornos altamente exigentes.- Tienes habilidades excepcionales de comunicación y sabes transmitir entusiasmo en el trabajo.- Eres una persona comprometida, con fuertes habilidades de análisis y resolución de problemas, así como habilidades de negociación.- Puedes establecer una conexión empática tanto con los compañeros de trabajo como con los clientes.- Te adaptas fácilmente a la cultura de trabajo de una empresa donde no hay grandes jerarquías y se valora la autonomía, la colaboración y el trabajo en equipo.- Estás interesado/a en desarrollar tu carrera en un entorno altamente competitivo con un gran potencial de crecimiento.

**En Bluetab, te ofrecemos:**
- Contrato indefinido, posterior al periodo de prueba, con un salario competitivo, basado en tus habilidades técnicas y el rol asignado, con posibilidad de ajustes conforme a tus evaluaciones de desempeño.- Formación continua.- Plan de carrera individualizado en áreas técnica, funcional o de gestión (¡Tú decides, pero siempre continuamos formándonos!).- Beneficios sociales que van más allá del salario, como un fondo de ahorro, seguros médicos para ti y tu familia, vales de despensa, vacaciones superiores a las establecidas por ley y más.

¿Qué estás esperando para unirte a Bluetab, an IBM Company? ¡Queremos saber de ti pronto!

**Muchas gracias por compartirme tu información, me interesa bastante tu perfil. Sin embargo, nuestro margen salarial solo nos permite ofertarte hasta $35,000 brutos, con nuestras prestaciones superiores de la ley como Vales de despensa de $2,047 al mes, Seguro de gastos médicos mayores y menores, 18 de vacaciones al año, Bolsa de capacitación de 10,000 al año, apoyo emocional de $1,500 al año, bono de bienestar de $15,000 también al año más convenios exclusivos de Bluetab.**


BLUETAB:

Empresa fundada en el año 2006 entre el reino unido y españa. Es una empresa multinacional especializada en consultoría tecnológica y soluciones avanzadas en el ámbito de los datos y la analítica. Se enfoca en ayudar a las grandes organizaciones a transformar sus procesos de gestión, integración y explotación de datos. 

En 2021 bluetab fue adquirida por IBM, lo cual terminó de consolidar su presencia a nivel internacional y he visto que trabajan muy de la mano con empresas como BBVA. 


### Present Yourself

Hi! My name is Andrés Basile, I am 25 years old. I was born in Argentina, but I have been living in Mexico for almost my whole life. I am a Computer Engineer passionate about data, data engineering and Artificial Intelligence, topics which I specialized on during my time at college at the National Autonomous University of Mexico, from which I graduated as the second best average of my generation. 

I currently work as a Presales Data Engineer which means that I am the middleground between the technical specialists and the commercial area of the company I work on. The company is called Edicom, which is a technology and SaaS solutions provider based off in Valencia, Spain,  where I lived and worked for a couple of months this year. My work mainly consists on translating business requirements into technical architectures. or transforming the need into a design which fits our current technical platform. 

At my time in college I gained foundational knowledge in software development, data engineering, and Artificial Intelligence, and I have also proactively taken steps to grow my expertise by developing my own projects, taking courses and consolidating my knowledge with certifications such as the Google cloud Professional data engineer certification. 

I am also a great advocate of what is called "learning in public", I currently write blog entries on my personal website, trying to transmit my experience and knowledge, and I think this translates perfectly into a business environment where it is crucial to be able to communicate our learnings, our wrongdoings and do so effectively to try and build a team of peers that can adapt and overcome any type of project. 

Looking ahead, I am ready to step into a full data egineering or data platform engineer role, where I can continue to grow my technical skills and eventually become an expert in the field. 
### Strengths
- Honesty: I firmly believe that it is better to say "I don't know" than to try and invent some answer. 
- Responsible: I am very responsible person, not only in the sense of fulfilling my obligations or responsibilities within the estimated time frame, but also in the sense of holding myself accountable for my mistakes. 
- Avid and fast learner: I love learning new topics, new technologies, working on new projects and I believe that I can do so very quickly, specially regarding the topics that I love most.  I tend to put the upmost passion into what I am doing and I believe this is a great catalyst for growth. 
### Weaknesses
- I believe that regarding this specific role, one of my weaknesses might be that I have limited formal experience in managing data platforms independently. However, I have been proactively seeking opportunities to develop these skills and I have even taken steps on my current role to try and make it more data and technically oriented by taking ownership of small initiatives like analyzing response times on our solutions, or studying our technical architecture to understand it and communicate it with my peers and even potential clients.
- Limited exposure to certain tools or platforms like NiFi, IDMC, or some AWS solutions. I view this as an opportunity for growth and I want to be able to apply these technologies in real-world projects. 

### Difficult time professionally 

From my experience working in technical and middle-ground positions, one of the most enduring professional challenges I’ve faced is aligning people and fostering collaboration to achieve a common goal. For example, on my current job, commercial and sales ideas might be far from technical possibilities, and having to frame a project taking into account these different mindsets can be challenging. However, I believe that there is a possibility of merging technical and business decisions by fostering open communication, setting clear expectations, and translating technical constraints into business-friendly language, and vice versa.  
### Cons of current job

The aspects I find least fulfilling about my current position are the limited opportunities for innovation and creative problem-solving. Additionally, I feel that the focus on monitoring minor details, such as punctuality, sometimes overshadows an emphasis on achieving impactful results. This has led me to seek a role where creativity, innovation, and results-oriented work are more highly valued and supported.











**Conocimientos**

- [ ] Compute Engine 
- [ ] App Engine
- [ ] Cloud Storage 
- [ ] BigQuery,
- [ ] Arquitectura de nube, 
- [ ] Incluyendo redes, virtualización
- [ ] Identidad, seguridad, business continuity 
- [ ] Recuperación de desastres y gobernanza de datos.
- [ ] Docker y Kubernetes.
- [ ] Conocimientos en bases de datos cloud y herramientas de replicación de datos
- [ ] Comprensión de principios de DevOps y herramientas asociadas (CI/CD, monitoreo, logging).
- [ ] Escalabilidad, rentabilidad
- [ ] Migracion local y otras nubes a GCP
- [ ] Terraform y Cloud Deployment Manager
- [ ] Optimizacion de costos
- [ ] Soporte tecnico en entornos de GCP



##### GCP Costos:
- Provisioned: Asegurate de poder manejar X cantidad de datos o procesamiento
- Por uso: Usaré esto y quiero que me cobres por esto. 

##### GCP Compute Services 

| Servicio                     | Descripcion                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| ---------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| GCE Google Compute Engine    | Let's you rent VM on demand (with OS<br>only). This is IaaS (Infrastructure as a Service) and it is similar to AWS EC2.<br>Custom CPU and RAM. Pay by the second (60 second min.). GCE has a sustained use<br>discount meaning that gets cheaper the more you use it. Cheaper for preemptible<br>(turn my VM off when they need the resource back). Live Migration: Google<br>seamlessly moves instance across hosts, as needed.                                  |
| GKE Google Kubernetes Engine | Managed Kubernetes cluster for<br>running Docker containers (with autoscaling). It was called Google Container Engine<br>(GKE). Comparable to AWS EC2 Container Service (ECS & EKS).<br>Integration with IAM<br>When you create a cluster,Google<br>automatically creates GCE instances (production cluster should have 3+ nodes, to<br>handle failures).                                                                                                         |
| Google App Engine            | Platform as a Service (PaaS) that<br>takes your code and runs it (similar to AWS Elastic Beanstalk or Heroku). Runs<br>almost any language. Auto-scaled based on load (NON FLEX mode can turn off last<br>instance when no traffic).                                                                                                                                                                                                                              |
| Google Cloud Run Functions   | Runs code in response to an event<br>(can be programmed in Node.js, Python, Java, Go). Functions as a Service (FaaS),<br>often referred to as Serverless. Similar to Lambda in AWS. Pay for CPU and RAM<br>assigned to function, per 100ms(min 100ms). Massively scalable (horizontally) - can<br>run many copies when needed. Useful when requests change a lot (not sure how many<br>people will use it), like chatbots, message processors, IoT or Automation. |

![[Pasted image 20250114101444.png]]

##### GCP Storage Services

| Service         | Description                                                                                                                                                                                                                   |     |
| --------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --- |
| Cloud Filestore | Fully managed file based storage<br>Comparable to AWS EFS or NAS<br>Primary use case is application migration to cloud<br>Backups and snapshots available<br>Support GKE workloads. Multiple pods can have shared file system |     |
| GCS             | Infinitely scalable<br>Versioned<br>Object Storage<br>Similar a S3<br>You can setup redundancy, regionality, nearline access, coldline access                                                                                 |     |

##### GCP Databases

| Service        | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Cloud SQL      | Fully-managed and reliable MySQL and PostgreSQL<br>databases. Similar to AWS RDS. Supports automatic replication, backup, failover.<br>Scaling is manual (vertically and horizontally). Effectively pay for underlying GCE<br>instances and PDs.                                                                                                                                                                                                                                                                                                                                     |
| Cloud Spanner  | If you outgrow cloud sql,<br>use cloud spanner, the first horizontally scalable, strongly consistent, relational<br>database service. Can use it as a normal mysql db. Minimum 3 nodes for production<br>environments. NODE is a server on each of the replication locations. CAP theorem<br>(check). Cloud spanner gives Consistency, Partition-tolerance and 99.999% (five<br>nines) availability. Not based on fail-over, any of the servers can handel any of<br>the requests. Pay for provisioned node time. USE for seriously big database<br>(thousands of dollars per month) |
| BigQuery       | Serverless column-store data warehouse for<br>analytics using SQL. If you dont use queries, you dont pay for queries, only for<br>the saved data. Scales internally, it can scan TB in seconds and PB in minutes.<br>Similar to AWS Athena. Pay for GBs actually considered (scanned) during queries.<br>Attempts to reuse cached results, free. Pay for GBs added via streaming inserts.                                                                                                                                                                                            |
| Cloud Bigtable | NoSQL low latency & high throughput nosql DB for<br>large operational and analytical apps. Similar to AWS DynamoDB or Apache Hbase.<br>Supports open source HBase API. Integrates with Hadoop, Dataflow, Dataproc. Scales<br>seamlessly. Pay for processing node hours, pay for GB-hours used for storage. Made<br>Useful for time series & IoT data, as well as finance data<br>for huge workloads, if not, consider Cloud Datastore                                                                                                                                                |

##### Data Transfer

| Service                  | Description                                                                                                                                                                                                                                                                                                  |
| ------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Data Transfer Appliance  | Rackable, high-capacity storage server to physically ship data to GCS. Similar to AWS snowball. Ingest only, not a way to avoid egress charges. 100TB or 480TB versions. 480TB/week is very fast. This is useful when your data is in your datacenter, if it is not, you could use Storage Transfer Service. |
| Storage Transfer Service | GLOBAL service. Copies objects, destination is always GCS bucket, source can be S3, HTTP/HTTPS endpoint, or another GCS bucket. Onte time or scheduled recurring transfers. Free to use, but you pay for its actions.                                                                                        |
![[Pasted image 20250108194936.png]]
![[Pasted image 20250108200056.png]]

##### Security

**Service accounts:** Help implement better security. Identity & Access Management works with Members and
Roles. Members can be a specific person, group of people or a service account on
the domain 12345@cloudservices.gserviceaccount.com. Roles can be, for example,
Instance Admin, Pub/Sub Publisher, Storage Object Viewer, etc.
Service Accounts are created for a specific non-human task requiring granular
authorization. Identity can be assumed by an application/workload... in the form of
keys, which can be easily rotated.
GCP services assume service account identity.

Fault tolerance and loosely coupling with PUBSUB

Integrate security into CI/CD PIPELINES: Automate security checks and vulnerability scanning within your development and deployment processes. 

Servicio DLP: Ayuda a descubrir, clasificar y proteger informacion sensible

KMS: Manejo de claves criptograficas para los servicios en la nube. 

**medidas de seguridad dependerán:**
- requerimientos de compliance
- Tipos de datos
- Complejidad de la arquitectura
- Budget. 

ISO 27001


##### GCP Dataproc
Dataproc is a great choice for quickly migrating Hadoop and Spark workloads
into GCP
- The biggest benefits of Dataproc over a self-managed Hadoop or Spark
cluster are the ease of scaling, being able to use cloud Storage instead of HDFS,
and the connectors to other GCP services like BigQuery, BigTable.



##### BigQuery

![[Pasted image 20250108211813.png]]

![[Pasted image 20250108211907.png]]
![[Pasted image 20250108212044.png]]

Optimize costs: 
partition data
indexing 
denormalise correctly
![[Pasted image 20250108212329.png]]
![[Pasted image 20250108212339.png]]
![[Pasted image 20250108212355.png]]

![[Pasted image 20250108212420.png]]

![[Pasted image 20250108212453.png]]



### Disaster recovery
Disaster recovery is a subset of business continuity planning. Disaster recovery planning begins with a business impact analysis that defines two key metrics:
- RTO (Recovery Time Objective), which is the maximum acceptable length of time that your application can be offline. 
- RPO (Recovery Point Objective), which is the maximum acceptable length of time during which data might be lost from your application due to a major incident. 

![[Pasted image 20250108222214.png]]
The smaller your RTO and RPO values, the more your application will cost to run. 

#### DR patterns
DR patterns are considered to be *cold*, *warm*, or *hot*. These patterns indicate how readily the system can recover when something goes wrong. 











## My Questions
- En qué tipo de proyectos estaría trabajando, principalmente? 
- Cuáles son las expectativas y métricas para evaluar el éxito en los primeros meses de trabajo? 
- Cómo está conformado el equipo de consultores que apoya en proyectos de ingeniería de datos con GCP? 




Bluetab: consultoría: 
- BBVA 
- Modelo de trabajo híbrido 2 a 3 veces por semana (torre polanco o torre bbva)
- Nube GCP
- conozca servicios y soluciones que maneja la nube
- migración de aplicaciones datos y servicios locales a GCP están manejando DATIUM? 
- Entrevista técnica y finalmente una entrevista con BBVA. 
- Procesos internos (prueba psicométrica)
- AVISO DE PRIVACIDAD y machote cv corporativo para editarlo con mi información profesional (en inglés o español). 
- Reporte de semanas cotizadas del IMSS. 
- Beneficios: 
	- Salario: 35k
	- Sueldo bruto total 35k - bonos de puntualidad y asistencia
	- vales de despensa 2000 pesos 
	- Préstamos personales via nómina sin intereses
	- Programas por referidos 
	- bonos repartición de utilidades 
	- 18 días al año
	- Días feriados de ley y blue days 24 y 31 de diciembre y medio dia en cumpleaños
	- seguro de vida y gastos medicos menores
	- bono de bienestar 15 mil pesos para gimnasio, lentes o cosas similares de bienestar
	- 1500 pesos para psicólogo jajajaja
	- 10 mil pesos para cursos y certificados 
	- becas educativas dependiendo de desempeño 




### ENTREVISTA TÉCNICA CON BLUETAB


### Resumen de Preguntas y Respuestas de la Preparación para Entrevista de GCP

#### **Pregunta 1: Diseña un pipeline de datos streaming en GCP**

**Pregunta:** ¿Cómo diseñarías un pipeline de datos en tiempo real que procese datos desde múltiples fuentes y almacene los resultados en BigQuery?

**Respuesta:**

1. Usar **Pub/Sub** para desacoplar las fuentes de datos y manejar la ingesta.
2. Configurar un job en **Dataflow** para procesar los datos en tiempo real. En Dataflow, implementar transformaciones como parsing, filtrado y agregaciones.
3. Configurar ventanas y marcas de agua para manejar datos tardíos.
4. Escribir los resultados procesados directamente en **BigQuery**.
5. Monitorear el pipeline con **Cloud Monitoring** y configurar alertas para identificar posibles fallas.

---

#### **Pregunta 2: ¿Cómo manejarías datos que llegan tarde a un sistema de análisis en tiempo real?**

**Respuesta:**

1. Implementar **ventanas de tiempo** (ventanas deslizantes o fijas) en Dataflow para segmentar los datos en intervalos procesables.
2. Utilizar **marcas de agua** para definir cuándo se considera que todos los datos relevantes de una ventana han llegado.
3. Configurar triggers para procesar datos tardíos, por ejemplo, acumulando los datos recibidos después del cierre inicial de la ventana.

---

#### **Pregunta 3: ¿Qué estrategias implementarías para reducir costos en BigQuery?**

**Respuesta:**

1. Evitar consultas del tipo `SELECT *`.
2. Usar **particiones** y **clústeres** para optimizar el almacenamiento y las consultas.
3. Revisar los bytes procesados antes de ejecutar consultas grandes.
4. Denormalizar datos en tablas siempre que sea posible.
5. Configurar vistas materializadas para consultas frecuentes.
6. Monitorear el uso con herramientas como Billing Reports.

---

#### **Pregunta 4: ¿Cómo establecerías un sistema de permisos para analistas en GCP?**

**Respuesta:**

1. Usar el sistema de **IAM (Identity and Access Management)** para asignar roles a los usuarios.
2. Crear un rol personalizado con permisos específicos (por ejemplo, acceso de solo lectura a ciertos datasets en BigQuery).
3. Implementar **políticas de acceso jerárquicas** para gestionar permisos a nivel de proyecto, carpeta o recurso.

---

#### **Pregunta 5: ¿Cómo manejarías el monitoreo y la optimización de costos en un entorno de GCP con múltiples servicios?**

**Respuesta:**

1. Implementar **Billing Budgets and Alerts** para definir límites de costos y recibir notificaciones.
2. Utilizar herramientas como **Billing Reports** para analizar el consumo por servicio.
3. Aplicar mejores prácticas en cada servicio:
    - BigQuery: particiones, clústeres, y vistas materializadas.
    - Dataflow: configuración de autoscaling y tuning de jobs.
    - Cloud Storage: elegir clases de almacenamiento según los patrones de acceso (Nearline, Coldline).
4. Monitorear recursos con **Cloud Monitoring**.

---

#### **Pregunta 6: ¿Cómo garantizarías la escalabilidad de una arquitectura de datos en GCP?**

**Respuesta:**

1. Usar **Dataflow** con autoscaling para ajustar los recursos según la carga de datos.
2. Implementar sistemas distribuidos como **BigQuery**, que escala automáticamente en procesamiento y almacenamiento.
3. Combinar con servicios como **GKE** para contenedores o **Cloud Run** para cargas serverless.
4. Supervisar el uso con **Cloud Monitoring** y ajustar configuraciones según las métricas.

---

#### **Pregunta 7: ¿Qué pasos tomarías para diseñar un plan de recuperación ante desastres en GCP?**

**Respuesta:**

1. Configurar backups regulares para servicios críticos como Cloud SQL y BigQuery.
2. Usar **replicación multirregional** en Cloud Storage para garantizar la disponibilidad de datos.
3. Definir un **Recovery Time Objective (RTO)** y un **Recovery Point Objective (RPO)** adecuados.
4. Implementar un plan de failover utilizando herramientas como **Cloud Spanner** o configuraciones de alta disponibilidad en Cloud SQL.

---

#### **Pregunta 8: ¿Cómo garantizarías una gobernanza de datos efectiva en GCP?**

**Respuesta:**

1. Cumplir con normativas locales e internacionales como GDPR o HIPAA.
2. Implementar **cifrado de datos en reposo** (AES-256) y en tránsito (TLS).
3. Usar **Cloud DLP** para identificar y proteger datos sensibles.
4. Definir roles y permisos con IAM y aplicar etiquetas para clasificación de datos.

---

#### **Pregunta 9: ¿Qué herramientas usarías para replicar datos entre entornos locales y GCP?**

**Respuesta:**

1. Usar **Database Migration Service** para migrar bases de datos relacionales.
2. Utilizar **Data Transfer Service** para mover datos hacia Cloud Storage.
3. Implementar soluciones de replicación en tiempo real con herramientas como **Datastream**.
4. Evaluar la arquitectura y los procesos actuales antes de seleccionar herramientas específicas.

---

#### **Pregunta 10: ¿Cómo administrarías máquinas virtuales en Compute Engine?**

**Respuesta:**

1. Seleccionar el tipo de máquina adecuado (series E2, N2, etc.) según los requerimientos de recursos.
2. Configurar **grupos de instancias administrados** para escalar automáticamente.
3. Usar **snapshots** para respaldos y restauración rápida de discos.
4. Configurar etiquetas y redes específicas para las VMs, asegurando una segmentación adecuada.
5. Implementar políticas de firewall y verificar la seguridad con **Cloud Armor**.

---

#### **Pregunta 11: ¿Cuándo utilizarías App Engine en lugar de Compute Engine o Cloud Run?**

**Respuesta:**

1. Usar **App Engine** para aplicaciones web o APIs que requieren despliegues rápidos y administración mínima.
2. Optar por App Engine cuando se busca escalabilidad automática integrada y no es necesario acceso directo al sistema operativo.
3. Elegir **Cloud Run** para aplicaciones containerizadas y **Compute Engine** para necesidades altamente personalizadas o control absoluto del entorno.

---

#### **Pregunta 12: ¿Qué estrategias aplicarías para optimizar costos en Cloud Storage?**

**Respuesta:**

1. Usar la clase de almacenamiento adecuada (Standard, Nearline, Coldline, o Archive) según la frecuencia de acceso.
2. Configurar **lifecycle rules** para mover datos a clases más económicas automáticamente.
3. Monitorear el uso con **Cloud Monitoring** y revisar el almacenamiento no utilizado.

---

#### **Pregunta 13: ¿Cómo implementarías redes virtuales y segmentación en GCP?**

**Respuesta:**

1. Crear redes VPC personalizadas con subredes específicas.
2. Configurar reglas de firewall para limitar el tráfico entrante y saliente.
3. Usar **Cloud NAT** para acceso seguro a internet desde instancias privadas.
4. Implementar **VPC peering** o **Interconnect** para conectar redes híbridas o multicloud.

---

#### **Pregunta 14: ¿Qué rol juega Kubernetes en la arquitectura en la nube?**

**Respuesta:**

1. Proporciona una plataforma para gestionar aplicaciones containerizadas.
2. Facilita la escalabilidad automática mediante **Horizontal Pod Autoscaler**.
3. Permite despliegues continuos y recuperación rápida de fallos.
4. Integrado con **GKE** para una gestión simplificada en GCP.

---

#### **Pregunta 15: ¿Cómo manejarías la continuidad del negocio y la recuperación en entornos críticos?**

**Respuesta:**

1. Configurar zonas y regiones redundantes para alta disponibilidad.
2. Implementar planes de failover y pruebas regulares de recuperación.
3. Asegurar backups automáticos de datos críticos.
4. Usar herramientas como **Cloud Operations Suite** para supervisar y responder ante incidentes.

---

#### **Pregunta 16: ¿Qué es Terraform y cómo lo usarías en GCP?**

**Respuesta:**

1. **Terraform** permite la gestión de infraestructura como código (IaC).
2. Facilita el despliegue reproducible de recursos como VMs, redes, y bases de datos en GCP.
3. Permite versionar la infraestructura y manejar cambios a través de pipelines CI/CD.
4. Alternativamente, usar **Cloud Deployment Manager** para manejar plantillas de recursos nativos.

---

#### **Pregunta 17: ¿Qué características tiene Google Compute Engine y cuándo lo usarías?**

**Respuesta:**

1. Google Compute Engine proporciona máquinas virtuales configurables para cargas de trabajo personalizadas.
2. Usar Compute Engine para ejecutar aplicaciones que requieren acceso directo al sistema operativo o bibliotecas específicas.
3. Configurar grupos de instancias administrados para escalar automáticamente según la demanda.
4. Integrar con discos persistentes y snapshots para recuperación de datos.

---

#### **Pregunta 18: ¿Cuándo elegirías Google App Engine sobre otros servicios?**

**Respuesta:**

1. Elegir App Engine para aplicaciones web donde se busca un entorno serverless con escalabilidad automática.
2. Usar App Engine cuando no se necesita acceso directo al sistema operativo ni configuraciones complejas.
3. Es ideal para aplicaciones con arquitecturas basadas en microservicios.

---

#### **Pregunta 19: ¿Qué es Google Cloud Run y cuándo es útil?**

**Respuesta:**

1. Google Cloud Run permite ejecutar contenedores de forma serverless.
2. Es útil para cargas de trabajo event-driven o aplicaciones de corta duración.
3. Escala automáticamente de 0 a N instancias, reduciendo costos para aplicaciones poco usadas.
4. Integra con Pub/Sub, Firestore y otros servicios GCP para aplicaciones modernas.

---

#### **Pregunta 20: ¿Qué ventajas ofrece Google Kubernetes Engine (GKE)?**

**Respuesta:**

1. GKE proporciona una plataforma completamente gestionada para ejecutar clústeres de Kubernetes.
2. Simplifica el escalado automático de pods y nodos según las métricas de uso.
3. Permite la integración con sistemas de CI/CD para despliegues rápidos y confiables.
4. Proporciona herramientas avanzadas de monitoreo e integración con Cloud Operations.