Open Source unified analytics engine for large-scale data processing. 

Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, maintained in a fault tolerant way. 

Apache Spark workflow is managed as a DAG, where Nodes represent RDDs and edges operations on RDDs. 

Apache Spark requires a cluster manager and a distributed storage system. 

Hadoop:





