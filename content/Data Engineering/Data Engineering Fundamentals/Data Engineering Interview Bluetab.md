---
draft: true
---

¬øQu√© buscamos?

**Conocimientos T√©cnicos:**
- Dominio de los servicios y soluciones de GCP (Compute Engine, App Engine, Cloud Storage, BigQuery, Looker).
- Experiencia en arquitectura de nube, incluyendo redes, virtualizaci√≥n, identidad, seguridad, business continuity, recuperaci√≥n de desastres y gobernanza de datos.
- Conocimientos en contenedores y su orquestaci√≥n.
_-_ Conocimientos en bases de datos cloud y herramientas de replicaci√≥n de datos
- Comprensi√≥n de principios de DevOps y herramientas asociadas (CI/CD, monitoreo, logging).

**Habilidades de Implementaci√≥n y Desarrollo:**
- Capacidad para dise√±ar, desarrollar e implementar soluciones escalables, seguras y rentables en GCP.
- Experiencia en la migraci√≥n de aplicaciones, datos y servicios desde entornos locales o de otras nubes a GCP.
- Conocimiento en el uso de herramientas de automatizaci√≥n e infraestructura como c√≥digo (Terraform, Cloud Deployment Manager).

**Certificaciones y Educaci√≥n:**
- Certificaciones relevantes de GCP, como Google Cloud Certified - Professional Cloud Architect o Google Cloud Certified - Professional Data Engineer.
- Formaci√≥n en inform√°tica, ingenier√≠a de sistemas o campos relacionados.

**Seguridad y Conformidad:**
- Experiencia en la configuraci√≥n y gesti√≥n de pol√≠ticas de seguridad en GCP.

**Optimizaci√≥n y Soporte:**
- Habilidad para optimizar costos y rendimiento de las soluciones en GCP.
- Experiencia en proporcionar soporte t√©cnico y resolver problemas en entornos de GCP.

**Se valorar√° de forma especial:**

- Apasionado de la tecnolog√≠a. Completamente al d√≠a de las √∫ltimas tendencias de la industria.- Habituado a trabajar por objetivos. Acostumbrado a trabajar bajo presi√≥n en entornos altamente exigentes.- Buen comunicador, sabr√° transmitir ilusi√≥n al entorno de trabajo.- Comprometido. Capacidad de an√°lisis y soluci√≥n de problemas.- Buenas habilidades de negociaci√≥n.- Capaz de generar empat√≠a tanto con los empleados y clientes.- Capaz de adaptarse a la cultura de trabajo de una pyme. Sin grandes jerarqu√≠as. Compatibilizando la capacidad de trabajar con autonom√≠a con un fuerte esp√≠ritu de colaboraci√≥n y, sobre todo, de trabajo en equipo.- Interesado en desarrollar su carrera profesional en entornos altamente competitivos y con alto potencial de crecimiento.

**Perfil del candidato/a ideal:**

- Eres un apasionado/a de la tecnolog√≠a y est√°s siempre al d√≠a con las √∫ltimas tendencias de la industria.- Est√°s acostumbrado/a a trabajar por objetivos y en entornos altamente exigentes.- Tienes habilidades excepcionales de comunicaci√≥n y sabes transmitir entusiasmo en el trabajo.- Eres una persona comprometida, con fuertes habilidades de an√°lisis y resoluci√≥n de problemas, as√≠ como habilidades de negociaci√≥n.- Puedes establecer una conexi√≥n emp√°tica tanto con los compa√±eros de trabajo como con los clientes.- Te adaptas f√°cilmente a la cultura de trabajo de una empresa donde no hay grandes jerarqu√≠as y se valora la autonom√≠a, la colaboraci√≥n y el trabajo en equipo.- Est√°s interesado/a en desarrollar tu carrera en un entorno altamente competitivo con un gran potencial de crecimiento.

**En Bluetab, te ofrecemos:**
- Contrato indefinido, posterior al periodo de prueba, con un salario competitivo, basado en tus habilidades t√©cnicas y el rol asignado, con posibilidad de ajustes conforme a tus evaluaciones de desempe√±o.- Formaci√≥n continua.- Plan de carrera individualizado en √°reas t√©cnica, funcional o de gesti√≥n (¬°T√∫ decides, pero siempre continuamos form√°ndonos!).- Beneficios sociales que van m√°s all√° del salario, como un fondo de ahorro, seguros m√©dicos para ti y tu familia, vales de despensa, vacaciones superiores a las establecidas por ley y m√°s.

¬øQu√© est√°s esperando para unirte a Bluetab, an IBM Company? ¬°Queremos saber de ti pronto!

**Muchas gracias por compartirme tu informaci√≥n, me interesa bastante tu perfil. Sin embargo, nuestro margen salarial solo nos permite ofertarte hasta $35,000 brutos, con nuestras prestaciones superiores de la ley como Vales de despensa de $2,047 al mes, Seguro de gastos m√©dicos mayores y menores, 18 de vacaciones al a√±o, Bolsa de capacitaci√≥n de 10,000 al a√±o, apoyo emocional de $1,500 al a√±o, bono de bienestar de $15,000 tambi√©n al a√±o m√°s convenios exclusivos de Bluetab.**


BLUETAB:

Empresa fundada en el a√±o 2006 entre el reino unido y espa√±a. Es una empresa multinacional especializada en consultor√≠a tecnol√≥gica y soluciones avanzadas en el √°mbito de los datos y la anal√≠tica. Se enfoca en ayudar a las grandes organizaciones a transformar sus procesos de gesti√≥n, integraci√≥n y explotaci√≥n de datos. 

En 2021 bluetab fue adquirida por IBM, lo cual termin√≥ de consolidar su presencia a nivel internacional y he visto que trabajan muy de la mano con empresas como BBVA. 


### Present Yourself

Hi! My name is Andr√©s Basile, I am 25 years old. I was born in Argentina, but I have been living in Mexico for almost my whole life. I am a Computer Engineer passionate about data, data engineering and Artificial Intelligence, topics which I specialized on during my time at college at the National Autonomous University of Mexico, from which I graduated as the second best average of my generation. 

I currently work as a Presales Data Engineer which means that I am the middleground between the technical specialists and the commercial area of the company I work on. The company is called Edicom, which is a technology and SaaS solutions provider based off in Valencia, Spain,  where I lived and worked for a couple of months this year. My work mainly consists on translating business requirements into technical architectures. or transforming the need into a design which fits our current technical platform. 

At my time in college I gained foundational knowledge in software development, data engineering, and Artificial Intelligence, and I have also proactively taken steps to grow my expertise by developing my own projects, taking courses and consolidating my knowledge with certifications such as the Google cloud Professional data engineer certification. 

I am also a great advocate of what is called "learning in public", I currently write blog entries on my personal website, trying to transmit my experience and knowledge, and I think this translates perfectly into a business environment where it is crucial to be able to communicate our learnings, our wrongdoings and do so effectively to try and build a team of peers that can adapt and overcome any type of project. 

Looking ahead, I am ready to step into a full data egineering or data platform engineer role, where I can continue to grow my technical skills and eventually become an expert in the field. 
### Strengths
- Honesty: I firmly believe that it is better to say "I don't know" than to try and invent some answer. 
- Responsible: I am very responsible person, not only in the sense of fulfilling my obligations or responsibilities within the estimated time frame, but also in the sense of holding myself accountable for my mistakes. 
- Avid and fast learner: I love learning new topics, new technologies, working on new projects and I believe that I can do so very quickly, specially regarding the topics that I love most.  I tend to put the upmost passion into what I am doing and I believe this is a great catalyst for growth. 
### Weaknesses
- I believe that regarding this specific role, one of my weaknesses might be that I have limited formal experience in managing data platforms independently. However, I have been proactively seeking opportunities to develop these skills and I have even taken steps on my current role to try and make it more data and technically oriented by taking ownership of small initiatives like analyzing response times on our solutions, or studying our technical architecture to understand it and communicate it with my peers and even potential clients.
- Limited exposure to certain tools or platforms like NiFi, IDMC, or some AWS solutions. I view this as an opportunity for growth and I want to be able to apply these technologies in real-world projects. 

### Difficult time professionally 

From my experience working in technical and middle-ground positions, one of the most enduring professional challenges I‚Äôve faced is aligning people and fostering collaboration to achieve a common goal. For example, on my current job, commercial and sales ideas might be far from technical possibilities, and having to frame a project taking into account these different mindsets can be challenging. However, I believe that there is a possibility of merging technical and business decisions by fostering open communication, setting clear expectations, and translating technical constraints into business-friendly language, and vice versa.  
### Cons of current job

The aspects I find least fulfilling about my current position are the limited opportunities for innovation and creative problem-solving. Additionally, I feel that the focus on monitoring minor details, such as punctuality, sometimes overshadows an emphasis on achieving impactful results. This has led me to seek a role where creativity, innovation, and results-oriented work are more highly valued and supported.











**Conocimientos**

- [ ] Compute Engine 
- [ ] App Engine
- [ ] Cloud Storage 
- [ ] BigQuery,
- [ ] Arquitectura de nube, 
- [ ] Incluyendo redes, virtualizaci√≥n
- [ ] Identidad, seguridad, business continuity 
- [ ] Recuperaci√≥n de desastres y gobernanza de datos.
- [ ] Docker y Kubernetes.
- [ ] Conocimientos en bases de datos cloud y herramientas de replicaci√≥n de datos
- [ ] Comprensi√≥n de principios de DevOps y herramientas asociadas (CI/CD, monitoreo, logging).
- [ ] Escalabilidad, rentabilidad
- [ ] Migracion local y otras nubes a GCP
- [ ] Terraform y Cloud Deployment Manager
- [ ] Optimizacion de costos
- [ ] Soporte tecnico en entornos de GCP



##### GCP Costos:
- Provisioned: Asegurate de poder manejar X cantidad de datos o procesamiento
- Por uso: Usar√© esto y quiero que me cobres por esto. 

##### GCP Compute Services 

| Servicio                     | Descripcion                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| ---------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| GCE Google Compute Engine    | Let's you rent VM on demand (with OS<br>only). This is IaaS (Infrastructure as a Service) and it is similar to AWS EC2.<br>Custom CPU and RAM. Pay by the second (60 second min.). GCE has a sustained use<br>discount meaning that gets cheaper the more you use it. Cheaper for preemptible<br>(turn my VM off when they need the resource back). Live Migration: Google<br>seamlessly moves instance across hosts, as needed.                                  |
| GKE Google Kubernetes Engine | Managed Kubernetes cluster for<br>running Docker containers (with autoscaling). It was called Google Container Engine<br>(GKE). Comparable to AWS EC2 Container Service (ECS & EKS).<br>Integration with IAM<br>When you create a cluster,Google<br>automatically creates GCE instances (production cluster should have 3+ nodes, to<br>handle failures).                                                                                                         |
| Google App Engine            | Platform as a Service (PaaS) that<br>takes your code and runs it (similar to AWS Elastic Beanstalk or Heroku). Runs<br>almost any language. Auto-scaled based on load (NON FLEX mode can turn off last<br>instance when no traffic).                                                                                                                                                                                                                              |
| Google Cloud Run Functions   | Runs code in response to an event<br>(can be programmed in Node.js, Python, Java, Go). Functions as a Service (FaaS),<br>often referred to as Serverless. Similar to Lambda in AWS. Pay for CPU and RAM<br>assigned to function, per 100ms(min 100ms). Massively scalable (horizontally) - can<br>run many copies when needed. Useful when requests change a lot (not sure how many<br>people will use it), like chatbots, message processors, IoT or Automation. |

![[Pasted image 20250114101444.png]]

##### GCP Storage Services

| Service         | Description                                                                                                                                                                                                                   |     |
| --------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --- |
| Cloud Filestore | Fully managed file based storage<br>Comparable to AWS EFS or NAS<br>Primary use case is application migration to cloud<br>Backups and snapshots available<br>Support GKE workloads. Multiple pods can have shared file system |     |
| GCS             | Infinitely scalable<br>Versioned<br>Object Storage<br>Similar a S3<br>You can setup redundancy, regionality, nearline access, coldline access                                                                                 |     |

##### GCP Databases

| Service        | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Cloud SQL      | Fully-managed and reliable MySQL and PostgreSQL<br>databases. Similar to AWS RDS. Supports automatic replication, backup, failover.<br>Scaling is manual (vertically and horizontally). Effectively pay for underlying GCE<br>instances and PDs.                                                                                                                                                                                                                                                                                                                                     |
| Cloud Spanner  | If you outgrow cloud sql,<br>use cloud spanner, the first horizontally scalable, strongly consistent, relational<br>database service. Can use it as a normal mysql db. Minimum 3 nodes for production<br>environments. NODE is a server on each of the replication locations. CAP theorem<br>(check). Cloud spanner gives Consistency, Partition-tolerance and 99.999% (five<br>nines) availability. Not based on fail-over, any of the servers can handel any of<br>the requests. Pay for provisioned node time. USE for seriously big database<br>(thousands of dollars per month) |
| BigQuery       | Serverless column-store data warehouse for<br>analytics using SQL. If you dont use queries, you dont pay for queries, only for<br>the saved data. Scales internally, it can scan TB in seconds and PB in minutes.<br>Similar to AWS Athena. Pay for GBs actually considered (scanned) during queries.<br>Attempts to reuse cached results, free. Pay for GBs added via streaming inserts.                                                                                                                                                                                            |
| Cloud Bigtable | NoSQL low latency & high throughput nosql DB for<br>large operational and analytical apps. Similar to AWS DynamoDB or Apache Hbase.<br>Supports open source HBase API. Integrates with Hadoop, Dataflow, Dataproc. Scales<br>seamlessly. Pay for processing node hours, pay for GB-hours used for storage. Made<br>Useful for time series & IoT data, as well as finance data<br>for huge workloads, if not, consider Cloud Datastore                                                                                                                                                |

##### Data Transfer

| Service                  | Description                                                                                                                                                                                                                                                                                                  |
| ------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Data Transfer Appliance  | Rackable, high-capacity storage server to physically ship data to GCS. Similar to AWS snowball. Ingest only, not a way to avoid egress charges. 100TB or 480TB versions. 480TB/week is very fast. This is useful when your data is in your datacenter, if it is not, you could use Storage Transfer Service. |
| Storage Transfer Service | GLOBAL service. Copies objects, destination is always GCS bucket, source can be S3, HTTP/HTTPS endpoint, or another GCS bucket. Onte time or scheduled recurring transfers. Free to use, but you pay for its actions.                                                                                        |
![[Pasted image 20250108194936.png]]
![[Pasted image 20250108200056.png]]

##### Security

**Service accounts:** Help implement better security. Identity & Access Management works with Members and
Roles. Members can be a specific person, group of people or a service account on
the domain 12345@cloudservices.gserviceaccount.com. Roles can be, for example,
Instance Admin, Pub/Sub Publisher, Storage Object Viewer, etc.
Service Accounts are created for a specific non-human task requiring granular
authorization. Identity can be assumed by an application/workload... in the form of
keys, which can be easily rotated.
GCP services assume service account identity.

Fault tolerance and loosely coupling with PUBSUB

Integrate security into CI/CD PIPELINES: Automate security checks and vulnerability scanning within your development and deployment processes. 

Servicio DLP: Ayuda a descubrir, clasificar y proteger informacion sensible

KMS: Manejo de claves criptograficas para los servicios en la nube. 

**medidas de seguridad depender√°n:**
- requerimientos de compliance
- Tipos de datos
- Complejidad de la arquitectura
- Budget. 

ISO 27001


##### GCP Dataproc
Dataproc is a great choice for quickly migrating Hadoop and Spark workloads
into GCP
- The biggest benefits of Dataproc over a self-managed Hadoop or Spark
cluster are the ease of scaling, being able to use cloud Storage instead of HDFS,
and the connectors to other GCP services like BigQuery, BigTable.



##### BigQuery

![[Pasted image 20250108211813.png]]

![[Pasted image 20250108211907.png]]
![[Pasted image 20250108212044.png]]

Optimize costs: 
partition data
indexing 
denormalise correctly
![[Pasted image 20250108212329.png]]
![[Pasted image 20250108212339.png]]
![[Pasted image 20250108212355.png]]

![[Pasted image 20250108212420.png]]

![[Pasted image 20250108212453.png]]



### Disaster recovery
Disaster recovery is a subset of business continuity planning. Disaster recovery planning begins with a business impact analysis that defines two key metrics:
- RTO (Recovery Time Objective), which is the maximum acceptable length of time that your application can be offline. 
- RPO (Recovery Point Objective), which is the maximum acceptable length of time during which data might be lost from your application due to a major incident. 

![[Pasted image 20250108222214.png]]
The smaller your RTO and RPO values, the more your application will cost to run. 

#### DR patterns
DR patterns are considered to be *cold*, *warm*, or *hot*. These patterns indicate how readily the system can recover when something goes wrong. 











## My Questions
- En qu√© tipo de proyectos estar√≠a trabajando, principalmente? 
- Cu√°les son las expectativas y m√©tricas para evaluar el √©xito en los primeros meses de trabajo? 
- C√≥mo est√° conformado el equipo de consultores que apoya en proyectos de ingenier√≠a de datos con GCP? 




Bluetab: consultor√≠a: 
- BBVA 
- Modelo de trabajo h√≠brido 2 a 3 veces por semana (torre polanco o torre bbva)
- Nube GCP
- conozca servicios y soluciones que maneja la nube
- migraci√≥n de aplicaciones datos y servicios locales a GCP est√°n manejando DATIUM? 
- Entrevista t√©cnica y finalmente una entrevista con BBVA. 
- Procesos internos (prueba psicom√©trica)
- AVISO DE PRIVACIDAD y machote cv corporativo para editarlo con mi informaci√≥n profesional (en ingl√©s o espa√±ol). 
- Reporte de semanas cotizadas del IMSS. 
- Beneficios: 
	- Salario: 35k
	- Sueldo bruto total 35k - bonos de puntualidad y asistencia
	- vales de despensa 2000 pesos 
	- Pr√©stamos personales via n√≥mina sin intereses
	- Programas por referidos 
	- bonos repartici√≥n de utilidades 
	- 18 d√≠as al a√±o
	- D√≠as feriados de ley y blue days 24 y 31 de diciembre y medio dia en cumplea√±os
	- seguro de vida y gastos medicos menores
	- bono de bienestar 15 mil pesos para gimnasio, lentes o cosas similares de bienestar
	- 1500 pesos para psic√≥logo jajajaja
	- 10 mil pesos para cursos y certificados 
	- becas educativas dependiendo de desempe√±o 




### ENTREVISTA T√âCNICA CON BLUETAB


### Resumen de Preguntas y Respuestas de la Preparaci√≥n para Entrevista de GCP

#### **Pregunta 1: Dise√±a un pipeline de datos streaming en GCP**

**Pregunta:** ¬øC√≥mo dise√±ar√≠as un pipeline de datos en tiempo real que procese datos desde m√∫ltiples fuentes y almacene los resultados en BigQuery?

**Respuesta:**

1. Usar **Pub/Sub** para desacoplar las fuentes de datos y manejar la ingesta.
2. Configurar un job en **Dataflow** para procesar los datos en tiempo real. En Dataflow, implementar transformaciones como parsing, filtrado y agregaciones.
3. Configurar ventanas y marcas de agua para manejar datos tard√≠os.
4. Escribir los resultados procesados directamente en **BigQuery**.
5. Monitorear el pipeline con **Cloud Monitoring** y configurar alertas para identificar posibles fallas.

---

#### **Pregunta 2: ¬øC√≥mo manejar√≠as datos que llegan tarde a un sistema de an√°lisis en tiempo real?**

**Respuesta:**

1. Implementar **ventanas de tiempo** (ventanas deslizantes o fijas) en Dataflow para segmentar los datos en intervalos procesables.
2. Utilizar **marcas de agua** para definir cu√°ndo se considera que todos los datos relevantes de una ventana han llegado.
3. Configurar triggers para procesar datos tard√≠os, por ejemplo, acumulando los datos recibidos despu√©s del cierre inicial de la ventana.

---

#### **Pregunta 3: ¬øQu√© estrategias implementar√≠as para reducir costos en BigQuery?**

**Respuesta:**

1. Evitar consultas del tipo `SELECT *`.
2. Usar **particiones** y **cl√∫steres** para optimizar el almacenamiento y las consultas.
3. Revisar los bytes procesados antes de ejecutar consultas grandes.
4. Denormalizar datos en tablas siempre que sea posible.
5. Configurar vistas materializadas para consultas frecuentes.
6. Monitorear el uso con herramientas como Billing Reports.

---

#### **Pregunta 4: ¬øC√≥mo establecer√≠as un sistema de permisos para analistas en GCP?**

**Respuesta:**

1. Usar el sistema de **IAM (Identity and Access Management)** para asignar roles a los usuarios.
2. Crear un rol personalizado con permisos espec√≠ficos (por ejemplo, acceso de solo lectura a ciertos datasets en BigQuery).
3. Implementar **pol√≠ticas de acceso jer√°rquicas** para gestionar permisos a nivel de proyecto, carpeta o recurso.

---

#### **Pregunta 5: ¬øC√≥mo manejar√≠as el monitoreo y la optimizaci√≥n de costos en un entorno de GCP con m√∫ltiples servicios?**

**Respuesta:**

1. Implementar **Billing Budgets and Alerts** para definir l√≠mites de costos y recibir notificaciones.
2. Utilizar herramientas como **Billing Reports** para analizar el consumo por servicio.
3. Aplicar mejores pr√°cticas en cada servicio:
    - BigQuery: particiones, cl√∫steres, y vistas materializadas.
    - Dataflow: configuraci√≥n de autoscaling y tuning de jobs.
    - Cloud Storage: elegir clases de almacenamiento seg√∫n los patrones de acceso (Nearline, Coldline).
4. Monitorear recursos con **Cloud Monitoring**.

---

#### **Pregunta 6: ¬øC√≥mo garantizar√≠as la escalabilidad de una arquitectura de datos en GCP?**

**Respuesta:**

1. Usar **Dataflow** con autoscaling para ajustar los recursos seg√∫n la carga de datos.
2. Implementar sistemas distribuidos como **BigQuery**, que escala autom√°ticamente en procesamiento y almacenamiento.
3. Combinar con servicios como **GKE** para contenedores o **Cloud Run** para cargas serverless.
4. Supervisar el uso con **Cloud Monitoring** y ajustar configuraciones seg√∫n las m√©tricas.

---

#### **Pregunta 7: ¬øQu√© pasos tomar√≠as para dise√±ar un plan de recuperaci√≥n ante desastres en GCP?**

**Respuesta:**

1. Configurar backups regulares para servicios cr√≠ticos como Cloud SQL y BigQuery.
2. Usar **replicaci√≥n multirregional** en Cloud Storage para garantizar la disponibilidad de datos.
3. Definir un **Recovery Time Objective (RTO)** y un **Recovery Point Objective (RPO)** adecuados.
4. Implementar un plan de failover utilizando herramientas como **Cloud Spanner** o configuraciones de alta disponibilidad en Cloud SQL.

---

#### **Pregunta 8: ¬øC√≥mo garantizar√≠as una gobernanza de datos efectiva en GCP?**

**Respuesta:**

1. Cumplir con normativas locales e internacionales como GDPR o HIPAA.
2. Implementar **cifrado de datos en reposo** (AES-256) y en tr√°nsito (TLS).
3. Usar **Cloud DLP** para identificar y proteger datos sensibles.
4. Definir roles y permisos con IAM y aplicar etiquetas para clasificaci√≥n de datos.

---

#### **Pregunta 9: ¬øQu√© herramientas usar√≠as para replicar datos entre entornos locales y GCP?**

**Respuesta:**

1. Usar **Database Migration Service** para migrar bases de datos relacionales.
2. Utilizar **Data Transfer Service** para mover datos hacia Cloud Storage.
3. Implementar soluciones de replicaci√≥n en tiempo real con herramientas como **Datastream**.
4. Evaluar la arquitectura y los procesos actuales antes de seleccionar herramientas espec√≠ficas.

---

#### **Pregunta 10: ¬øC√≥mo administrar√≠as m√°quinas virtuales en Compute Engine?**

**Respuesta:**

1. Seleccionar el tipo de m√°quina adecuado (series E2, N2, etc.) seg√∫n los requerimientos de recursos.
2. Configurar **grupos de instancias administrados** para escalar autom√°ticamente.
3. Usar **snapshots** para respaldos y restauraci√≥n r√°pida de discos.
4. Configurar etiquetas y redes espec√≠ficas para las VMs, asegurando una segmentaci√≥n adecuada.
5. Implementar pol√≠ticas de firewall y verificar la seguridad con **Cloud Armor**.

---

#### **Pregunta 11: ¬øCu√°ndo utilizar√≠as App Engine en lugar de Compute Engine o Cloud Run?**

**Respuesta:**

1. Usar **App Engine** para aplicaciones web o APIs que requieren despliegues r√°pidos y administraci√≥n m√≠nima.
2. Optar por App Engine cuando se busca escalabilidad autom√°tica integrada y no es necesario acceso directo al sistema operativo.
3. Elegir **Cloud Run** para aplicaciones containerizadas y **Compute Engine** para necesidades altamente personalizadas o control absoluto del entorno.

---

#### **Pregunta 12: ¬øQu√© estrategias aplicar√≠as para optimizar costos en Cloud Storage?**

**Respuesta:**

1. Usar la clase de almacenamiento adecuada (Standard, Nearline, Coldline, o Archive) seg√∫n la frecuencia de acceso.
2. Configurar **lifecycle rules** para mover datos a clases m√°s econ√≥micas autom√°ticamente.
3. Monitorear el uso con **Cloud Monitoring** y revisar el almacenamiento no utilizado.

---

#### **Pregunta 13: ¬øC√≥mo implementar√≠as redes virtuales y segmentaci√≥n en GCP?**

**Respuesta:**

1. Crear redes VPC personalizadas con subredes espec√≠ficas.
2. Configurar reglas de firewall para limitar el tr√°fico entrante y saliente.
3. Usar **Cloud NAT** para acceso seguro a internet desde instancias privadas.
4. Implementar **VPC peering** o **Interconnect** para conectar redes h√≠bridas o multicloud.

---

#### **Pregunta 14: ¬øQu√© rol juega Kubernetes en la arquitectura en la nube?**

**Respuesta:**

1. Proporciona una plataforma para gestionar aplicaciones containerizadas.
2. Facilita la escalabilidad autom√°tica mediante **Horizontal Pod Autoscaler**.
3. Permite despliegues continuos y recuperaci√≥n r√°pida de fallos.
4. Integrado con **GKE** para una gesti√≥n simplificada en GCP.

---

#### **Pregunta 15: ¬øC√≥mo manejar√≠as la continuidad del negocio y la recuperaci√≥n en entornos cr√≠ticos?**

**Respuesta:**

1. Configurar zonas y regiones redundantes para alta disponibilidad.
2. Implementar planes de failover y pruebas regulares de recuperaci√≥n.
3. Asegurar backups autom√°ticos de datos cr√≠ticos.
4. Usar herramientas como **Cloud Operations Suite** para supervisar y responder ante incidentes.

---

#### **Pregunta 16: ¬øQu√© es Terraform y c√≥mo lo usar√≠as en GCP?**

**Respuesta:**

1. **Terraform** permite la gesti√≥n de infraestructura como c√≥digo (IaC).
2. Facilita el despliegue reproducible de recursos como VMs, redes, y bases de datos en GCP.
3. Permite versionar la infraestructura y manejar cambios a trav√©s de pipelines CI/CD.
4. Alternativamente, usar **Cloud Deployment Manager** para manejar plantillas de recursos nativos.

---

#### **Pregunta 17: ¬øQu√© caracter√≠sticas tiene Google Compute Engine y cu√°ndo lo usar√≠as?**

**Respuesta:**

1. Google Compute Engine proporciona m√°quinas virtuales configurables para cargas de trabajo personalizadas.
2. Usar Compute Engine para ejecutar aplicaciones que requieren acceso directo al sistema operativo o bibliotecas espec√≠ficas.
3. Configurar grupos de instancias administrados para escalar autom√°ticamente seg√∫n la demanda.
4. Integrar con discos persistentes y snapshots para recuperaci√≥n de datos.

---

#### **Pregunta 18: ¬øCu√°ndo elegir√≠as Google App Engine sobre otros servicios?**

**Respuesta:**

1. Elegir App Engine para aplicaciones web donde se busca un entorno serverless con escalabilidad autom√°tica.
2. Usar App Engine cuando no se necesita acceso directo al sistema operativo ni configuraciones complejas.
3. Es ideal para aplicaciones con arquitecturas basadas en microservicios.

---

#### **Pregunta 19: ¬øQu√© es Google Cloud Run y cu√°ndo es √∫til?**

**Respuesta:**

1. Google Cloud Run permite ejecutar contenedores de forma serverless.
2. Es √∫til para cargas de trabajo event-driven o aplicaciones de corta duraci√≥n.
3. Escala autom√°ticamente de 0 a N instancias, reduciendo costos para aplicaciones poco usadas.
4. Integra con Pub/Sub, Firestore y otros servicios GCP para aplicaciones modernas.

---

#### **Pregunta 20: ¬øQu√© ventajas ofrece Google Kubernetes Engine (GKE)?**

**Respuesta:**

1. GKE proporciona una plataforma completamente gestionada para ejecutar cl√∫steres de Kubernetes.
2. Simplifica el escalado autom√°tico de pods y nodos seg√∫n las m√©tricas de uso.
3. Permite la integraci√≥n con sistemas de CI/CD para despliegues r√°pidos y confiables.
4. Proporciona herramientas avanzadas de monitoreo e integraci√≥n con Cloud Operations.






REVISI√ìN CARTA OFERTA


- ¬øCu√°l ser√≠a el horario final? 
- En alg√∫n lado se asegura la modalidad de trabajo? 

- El primer d√≠a de trabajo, me presentar√≠a directamente con BBVA? o con ustedes? 

- Una vez firmando la carta oferta, ya est√° asegurada la posici√≥n y podr√≠a renunciar a mi empleo actual, verdad? Cu√°ndo har√≠amos la firma del contrato final?
- La constancia laboral de √∫ltimo empleo, qu√© documento es?
- Cuentan con estacionamiento en las oficinas? 



Los documentos ya pr√°cticamente los tengo todos listos
Las pruebas que me enviaste hace un rato, ya avanc√© bastante, pero todav√≠a me falta un poco. 


Hola, Les! 
Te ped√≠ este espacio para platicar porque durante la semana pasada se acercaron conmigo de la empresa Bluetab para participar en un proyecto de Inteligencia artificial generativa en el banco BBVA. Me llamaron el martes pasado y me entrevistaron en esos d√≠as. La verdad fue muy repentino todo. He decidido que voy a tomar la oferta que me est√°n haciendo y que voy a renunciar a edicom. 




¬°Hola a todos! Mi nombre es Andr√©s Basile. Tengo 25 a√±os y soy ingeniero en computaci√≥n de la UNAM. 
¬°Estoy muy emocionado de unirme a Bluetab! 
Un poquito m√°s sobre m√≠: Me encanta la m√∫sica y tocar la guitarra, disfruto del deporte, caminar en la naturaleza y, por supuesto, compartir buenos momentos con amigos. 
¬°Espero aprender mucho, aportar lo mejor de m√≠ y conocerlos a todos!  ![üöÄ](https://fonts.gstatic.com/s/e/notoemoji/16.0/1f680/32.png)  

  

Saludos,